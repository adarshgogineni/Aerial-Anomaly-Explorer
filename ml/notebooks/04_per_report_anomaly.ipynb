{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UAP Explorer - Per-Report Anomaly Detection\n",
    "\n",
    "This notebook combines all previous work to generate final per-report anomaly scores.\n",
    "\n",
    "## Objectives\n",
    "1. Merge cleaned sightings with spatiotemporal anomalies and text clusters\n",
    "2. Engineer features combining all data sources\n",
    "3. Train Isolation Forest for per-report anomaly detection\n",
    "4. Generate final anomaly scores (0-1 scale)\n",
    "5. Export complete dataset ready for frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned sightings\n",
    "print(\"Loading data sources...\\n\")\n",
    "\n",
    "cleaned_path = Path('../data/processed/cleaned_sightings.parquet')\n",
    "df = pd.read_parquet(cleaned_path)\n",
    "print(f\"‚úì Loaded {len(df):,} cleaned sightings\")\n",
    "\n",
    "# Load cluster assignments\n",
    "clusters_path = Path('../data/processed/sightings_with_clusters.parquet')\n",
    "if clusters_path.exists():\n",
    "    df_clusters = pd.read_parquet(clusters_path)\n",
    "    print(f\"‚úì Loaded {len(df_clusters):,} cluster assignments\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cluster data not found. Please run 03_text_clusters.ipynb first.\")\n",
    "    df_clusters = None\n",
    "\n",
    "# Load spatiotemporal anomalies\n",
    "grid_anomalies_path = Path('../data/processed/grid_time_anomalies.parquet')\n",
    "if grid_anomalies_path.exists():\n",
    "    df_grid_anomalies = pd.read_parquet(grid_anomalies_path)\n",
    "    print(f\"‚úì Loaded {len(df_grid_anomalies):,} grid-time anomaly scores\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Grid anomaly data not found. Please run 02_spatiotemporal_baseline.ipynb first.\")\n",
    "    df_grid_anomalies = None\n",
    "\n",
    "# Load cluster labels\n",
    "labels_path = Path('../data/processed/cluster_labels.json')\n",
    "if labels_path.exists():\n",
    "    with open(labels_path) as f:\n",
    "        cluster_labels = json.load(f)\n",
    "    # Convert keys to integers\n",
    "    cluster_labels = {int(k): v for k, v in cluster_labels.items()}\n",
    "    print(f\"‚úì Loaded {len(cluster_labels)} cluster labels\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cluster labels not found\")\n",
    "    cluster_labels = None\n",
    "\n",
    "print(f\"\\nInitial dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge All Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Merge clusters\nif df_clusters is not None:\n    df = df.merge(df_clusters[['id', 'cluster_id', 'cluster_label']], on='id', how='left')\n    print(f\"‚úì Merged cluster data\")\n    print(f\"  Records with cluster: {df['cluster_id'].notna().sum():,}\")\nelse:\n    # Create dummy cluster\n    df['cluster_id'] = 0\n    df['cluster_label'] = 'unknown'\n    print(\"‚ö†Ô∏è  Using dummy clusters (all set to 0)\")\n\n# Merge spatiotemporal anomalies\nif df_grid_anomalies is not None:\n    # Convert year_month to string for matching (both dataframes)\n    df['year_month_str'] = df['year_month'].astype(str)\n    df_grid_anomalies['year_month_str'] = df_grid_anomalies['year_month'].astype(str)\n    \n    # Merge on grid_id and year_month_str\n    merge_cols = ['grid_id', 'year_month_str', 'anomaly_score_cell', 'predicted_count']\n    df_grid_subset = df_grid_anomalies[merge_cols]\n    \n    df = df.merge(\n        df_grid_subset,\n        on=['grid_id', 'year_month_str'],\n        how='left',\n        suffixes=('', '_grid')\n    )\n    \n    # Fill NaN anomaly scores with 0 (for cells without enough data)\n    df['anomaly_score_cell'] = df['anomaly_score_cell'].fillna(0)\n    df['predicted_count'] = df['predicted_count'].fillna(df['predicted_count'].mean())\n    \n    print(f\"‚úì Merged spatiotemporal anomalies\")\n    print(f\"  Records with cell anomaly score: {df['anomaly_score_cell'].notna().sum():,}\")\nelse:\n    # Create dummy anomaly scores\n    df['anomaly_score_cell'] = 0\n    df['predicted_count'] = 1\n    print(\"‚ö†Ô∏è  Using dummy cell anomaly scores (all set to 0)\")\n\nprint(f\"\\nMerged dataset shape: {df.shape}\")\nprint(f\"\\nSample merged data:\")\ndf[['id', 'datetime', 'location', 'cluster_label', 'anomaly_score_cell']].head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering for Anomaly Detection\n",
    "\n",
    "Create features combining:\n",
    "- Temporal: hour, day_of_week, month\n",
    "- Duration: normalized duration\n",
    "- Spatial: cell anomaly score\n",
    "- Text: cluster ID (one-hot encoded)\n",
    "- Shape: shape category (one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "print(\"Engineering features for anomaly detection...\\n\")\n",
    "\n",
    "features_list = []\n",
    "\n",
    "# 1. Duration (log-transformed to handle outliers)\n",
    "df['duration_log'] = np.log1p(df['duration_seconds'])\n",
    "features_list.append('duration_log')\n",
    "print(\"‚úì Added duration_log\")\n",
    "\n",
    "# 2. Time of day\n",
    "features_list.append('hour')\n",
    "print(\"‚úì Added hour\")\n",
    "\n",
    "# 3. Day of week\n",
    "features_list.append('day_of_week')\n",
    "print(\"‚úì Added day_of_week\")\n",
    "\n",
    "# 4. Month (seasonality)\n",
    "features_list.append('month')\n",
    "print(\"‚úì Added month\")\n",
    "\n",
    "# 5. Cell anomaly score\n",
    "features_list.append('anomaly_score_cell')\n",
    "print(\"‚úì Added anomaly_score_cell\")\n",
    "\n",
    "# 6. Description length (normalized)\n",
    "df['description_length_norm'] = df['description_length'] / df['description_length'].max()\n",
    "features_list.append('description_length_norm')\n",
    "print(\"‚úì Added description_length_norm\")\n",
    "\n",
    "# 7. Cluster ID (one-hot encoded)\n",
    "cluster_dummies = pd.get_dummies(df['cluster_id'], prefix='cluster')\n",
    "cluster_cols = cluster_dummies.columns.tolist()\n",
    "df = pd.concat([df, cluster_dummies], axis=1)\n",
    "features_list.extend(cluster_cols)\n",
    "print(f\"‚úì Added {len(cluster_cols)} cluster features\")\n",
    "\n",
    "# 8. Shape (one-hot encoded) - limit to top shapes\n",
    "top_shapes = df['shape'].value_counts().head(10).index.tolist()\n",
    "df['shape_category'] = df['shape'].apply(lambda x: x if x in top_shapes else 'other')\n",
    "shape_dummies = pd.get_dummies(df['shape_category'], prefix='shape')\n",
    "shape_cols = shape_dummies.columns.tolist()\n",
    "df = pd.concat([df, shape_dummies], axis=1)\n",
    "features_list.extend(shape_cols)\n",
    "print(f\"‚úì Added {len(shape_cols)} shape features\")\n",
    "\n",
    "print(f\"\\nTotal features: {len(features_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix\n",
    "X = df[features_list].copy()\n",
    "\n",
    "# Handle any remaining NaN values\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"\\nFeature summary:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Isolation Forest for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for better anomaly detection\n",
    "print(\"Standardizing features...\\n\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"‚úì Features standardized\")\n",
    "print(f\"  Shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Isolation Forest\n",
    "print(\"Training Isolation Forest...\\n\")\n",
    "\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.1,  # Expect ~10% anomalies\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_samples='auto',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "anomaly_predictions = iso_forest.fit_predict(X_scaled)\n",
    "\n",
    "# Get anomaly scores (lower is more anomalous)\n",
    "anomaly_scores_raw = iso_forest.score_samples(X_scaled)\n",
    "\n",
    "print(\"\\n‚úì Isolation Forest training complete\")\n",
    "print(f\"  Anomalies detected: {(anomaly_predictions == -1).sum():,} ({(anomaly_predictions == -1).sum()/len(anomaly_predictions)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize anomaly scores to 0-1 range\n",
    "# (Higher score = more anomalous)\n",
    "scaler_anomaly = MinMaxScaler()\n",
    "# Negate scores so higher = more anomalous\n",
    "anomaly_scores_normalized = scaler_anomaly.fit_transform(\n",
    "    -anomaly_scores_raw.reshape(-1, 1)\n",
    ").flatten()\n",
    "\n",
    "# Add to dataframe\n",
    "df['anomaly_score_report'] = anomaly_scores_normalized\n",
    "df['is_anomaly'] = (anomaly_predictions == -1).astype(int)\n",
    "\n",
    "print(\"‚úì Anomaly scores normalized to 0-1 range\")\n",
    "print(f\"\\nAnomaly score statistics:\")\n",
    "print(df['anomaly_score_report'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomaly score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['anomaly_score_report'], bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Anomaly Score (0-1)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Per-Report Anomaly Scores')\n",
    "axes[0].axvline(df['anomaly_score_report'].median(), color='red', linestyle='--', \n",
    "                label=f'Median: {df[\"anomaly_score_report\"].median():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cumulative distribution\n",
    "sorted_scores = np.sort(df['anomaly_score_report'])\n",
    "cumulative = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores)\n",
    "axes[1].plot(sorted_scores, cumulative, linewidth=2)\n",
    "axes[1].set_xlabel('Anomaly Score')\n",
    "axes[1].set_ylabel('Cumulative Probability')\n",
    "axes[1].set_title('Cumulative Distribution of Anomaly Scores')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top anomalies\n",
    "print(\"üî• TOP 20 MOST ANOMALOUS SIGHTINGS\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "top_anomalies = df.nlargest(20, 'anomaly_score_report')[[\n",
    "    'datetime', 'location', 'shape', 'duration_seconds',\n",
    "    'cluster_label', 'anomaly_score_cell', 'anomaly_score_report', 'description'\n",
    "]]\n",
    "\n",
    "for idx, row in top_anomalies.iterrows():\n",
    "    print(f\"\\nScore: {row['anomaly_score_report']:.3f} | {row['datetime']} | {row['location']}\")\n",
    "    print(f\"Shape: {row['shape']} | Duration: {row['duration_seconds']:.0f}s | Cluster: {row['cluster_label']}\")\n",
    "    print(f\"Cell Anomaly: {row['anomaly_score_cell']:.2f}\")\n",
    "    print(f\"Description: {row['description'][:150]}...\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly score by cluster\n",
    "cluster_anomalies = df.groupby('cluster_label')['anomaly_score_report'].agg(['mean', 'median', 'max', 'count'])\n",
    "cluster_anomalies = cluster_anomalies.sort_values('mean', ascending=False)\n",
    "\n",
    "print(\"\\nAverage Anomaly Score by Cluster:\")\n",
    "print(cluster_anomalies)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_anomalies['mean'].plot(kind='barh')\n",
    "plt.xlabel('Average Anomaly Score')\n",
    "plt.title('Average Anomaly Score by Description Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between cell anomaly and report anomaly\n",
    "correlation = df[['anomaly_score_cell', 'anomaly_score_report']].corr()\n",
    "print(\"\\nCorrelation between Cell and Report Anomaly Scores:\")\n",
    "print(correlation)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['anomaly_score_cell'], df['anomaly_score_report'], alpha=0.1, s=1)\n",
    "plt.xlabel('Cell Anomaly Score (Spatiotemporal)')\n",
    "plt.ylabel('Report Anomaly Score (Per-Sighting)')\n",
    "plt.title('Relationship Between Cell and Report Anomaly Scores')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Final Dataset\n",
    "\n",
    "Create the final dataset with all information ready for the frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for final export\n",
    "final_columns = [\n",
    "    # Identifiers\n",
    "    'id',\n",
    "    \n",
    "    # Datetime\n",
    "    'datetime', 'year', 'month', 'day_of_week', 'hour',\n",
    "    \n",
    "    # Location\n",
    "    'city', 'state', 'country', 'location',\n",
    "    'latitude', 'longitude',\n",
    "    'grid_lat', 'grid_lon', 'grid_id',\n",
    "    \n",
    "    # Shape and duration\n",
    "    'shape', 'duration_seconds',\n",
    "    \n",
    "    # Text\n",
    "    'description', 'description_length',\n",
    "    \n",
    "    # Clusters\n",
    "    'cluster_id', 'cluster_label',\n",
    "    \n",
    "    # Anomaly scores\n",
    "    'anomaly_score_cell',      # Spatiotemporal anomaly\n",
    "    'anomaly_score_report',    # Per-report anomaly (MAIN SCORE)\n",
    "    'is_anomaly',              # Binary flag\n",
    "    \n",
    "    # Additional\n",
    "    'date posted'\n",
    "]\n",
    "\n",
    "# Create final dataframe\n",
    "df_final = df[final_columns].copy()\n",
    "\n",
    "# Sort by anomaly score (highest first)\n",
    "df_final = df_final.sort_values('anomaly_score_report', ascending=False)\n",
    "\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"\\nColumn list:\")\n",
    "for i, col in enumerate(final_columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to parquet\n",
    "output_path = Path('../data/processed/sightings_with_scores.parquet')\n",
    "df_final.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"‚úì Exported final dataset to: {output_path}\")\n",
    "print(f\"  Records: {len(df_final):,}\")\n",
    "print(f\"  Columns: {len(final_columns)}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# Also export to CSV for easy inspection\n",
    "csv_path = output_path.with_suffix('.csv')\n",
    "df_final.head(1000).to_csv(csv_path, index=False)\n",
    "print(f\"‚úì Exported top 1000 to CSV: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Total sightings: {len(df_final):,}\")\n",
    "print(f\"  Date range: {df_final['year'].min():.0f} - {df_final['year'].max():.0f}\")\n",
    "print(f\"  Countries: {df_final['country'].nunique()}\")\n",
    "print(f\"  Unique locations: {df_final['location'].nunique():,}\")\n",
    "print(f\"  Grid cells: {df_final['grid_id'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nüî∑ Clusters:\")\n",
    "print(f\"  Total clusters: {df_final['cluster_id'].nunique()}\")\n",
    "cluster_dist = df_final['cluster_label'].value_counts().head(5)\n",
    "for label, count in cluster_dist.items():\n",
    "    print(f\"  ‚Ä¢ {label}: {count:,} sightings\")\n",
    "\n",
    "print(f\"\\nüéØ Anomaly Scores:\")\n",
    "print(f\"  Mean: {df_final['anomaly_score_report'].mean():.3f}\")\n",
    "print(f\"  Median: {df_final['anomaly_score_report'].median():.3f}\")\n",
    "print(f\"  95th percentile: {df_final['anomaly_score_report'].quantile(0.95):.3f}\")\n",
    "print(f\"  99th percentile: {df_final['anomaly_score_report'].quantile(0.99):.3f}\")\n",
    "print(f\"  Flagged as anomaly: {df_final['is_anomaly'].sum():,} ({df_final['is_anomaly'].sum()/len(df_final)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"‚úÖ TASK 2.5 COMPLETE! ALL ML PIPELINE TASKS DONE!\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nüéâ Phase 2 Complete!\\n\")\n",
    "print(\"Next steps:\")\n",
    "print(\"  1. Review the final dataset: data/processed/sightings_with_scores.parquet\")\n",
    "print(\"  2. Proceed to Task 3.1: Design tile format & zoom strategy\")\n",
    "print(\"  3. Then Task 3.2: Implement export script to generate frontend data\")\n",
    "print(\"\\nThe dataset is now ready to be exported as JSON tiles for the frontend!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}