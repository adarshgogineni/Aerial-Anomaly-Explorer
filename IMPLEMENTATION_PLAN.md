# üöÄ UAP Explorer - Implementation Plan & Progress Tracker

> **Last Updated**: 2025-11-26
> **Version**: 1.0
> **Status**: Phase 1 Complete, Phase 2 In Progress

This is your comprehensive, step-by-step guide to building the UAP Explorer application. Check off tasks as you complete them to track your progress.

---

## üìä Overall Progress

```
Phase 1: Setup & Scaffolding           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% (9/9)   ‚úÖ COMPLETE
Phase 2: Data & ML Pipeline            ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   0% (0/6)   üîÑ CURRENT
Phase 3: Static Data Export            ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   0% (0/2)   ‚è∏Ô∏è PENDING
Phase 4: Frontend Map & Filters        ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   0% (0/5)   ‚è∏Ô∏è PENDING
Phase 5: Detail Panel & Polish         ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   0% (0/4)   ‚è∏Ô∏è PENDING
Phase 6: Testing & Deployment          ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   0% (0/4)   ‚è∏Ô∏è PENDING

TOTAL PROGRESS: 30% (9/30 tasks)
```

---

## ‚úÖ Phase 1: Repo Setup & Scaffolding (COMPLETED)

**Status**: ‚úÖ Complete
**Time Invested**: ~1 hour
**Progress**: 9/9 tasks complete

### Completed Tasks:

- [x] **Task 1.1**: Create project directory structure
  - Created `app/` and `ml/` directories
  - Created subdirectories: `ml/data/raw/`, `ml/data/processed/`, `ml/notebooks/`, `ml/scripts/`
  - Created `app/components/`, `app/public/data/tiles/`, `app/public/data/metadata/`

- [x] **Task 1.2**: Initialize Next.js app with TypeScript
  - Created `package.json` with Next.js 15, React 19, TypeScript
  - Created `tsconfig.json` with strict mode
  - Created `next.config.ts`

- [x] **Task 1.3**: Configure Tailwind CSS
  - Created `tailwind.config.ts`
  - Created `postcss.config.mjs`
  - Set up `app/globals.css` with Tailwind directives
  - Configured dark mode support

- [x] **Task 1.4**: Create basic layout and pages
  - Created `app/layout.tsx` with header
  - Created `app/page.tsx` with homepage content
  - ESLint configured

- [x] **Task 1.5**: Setup Python environment
  - Created `ml/requirements.txt` with all ML dependencies
  - Created `ml/README.md` with setup instructions

- [x] **Task 1.6**: Create initial exploration notebook
  - Created `ml/notebooks/01_explore_data.ipynb` template

- [x] **Task 1.7**: Configure git and gitignore
  - Created comprehensive `.gitignore` for Node.js and Python

- [x] **Task 1.8**: Create documentation
  - Updated root `README.md` with architecture and quick start
  - Created `SETUP_COMPLETE.md` verification checklist

- [x] **Task 1.9**: Install dependencies and verify build
  - Ran `npm install` successfully
  - Verified `npm run build` works without errors

---

## üîÑ Phase 2: Data Ingestion & Offline ML Pipeline

**Status**: üîÑ Current Phase
**Estimated Time**: 8-12 hours
**Progress**: 0/6 tasks complete

### Task 2.1: Download and Inspect UAP Dataset

**Estimated Time**: 30 minutes

#### Checklist:
- [ ] Download UAP/UFO sighting dataset
- [ ] Place dataset CSV in `ml/data/raw/`
- [ ] Verify dataset has required columns
- [ ] Update filename in exploration notebook

#### üìÅ Files Affected:
- `ml/data/raw/[your-dataset].csv` (new)
- `ml/notebooks/01_explore_data.ipynb` (update)

#### üí° Dataset Sources:
```bash
# Option 1: NUFORC (National UFO Reporting Center)
# Visit: https://nuforc.org/webreports/
# Download scrubbed report database

# Option 2: Kaggle
# Search: "UFO sightings dataset"
# Example: https://www.kaggle.com/datasets/NUFORC/ufo-sightings

# Option 3: GitHub repositories
# Search: "UFO sightings CSV" or "UAP data"
```

#### üéØ Acceptance Criteria:
- [ ] Dataset file exists in `ml/data/raw/`
- [ ] Contains minimum columns: date/time, location, lat/lon, description, shape
- [ ] At least 10,000+ records (ideally 50k-100k)
- [ ] CSV is readable by pandas

#### ‚úÖ Verification Steps:
```bash
cd ml
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
jupyter notebook notebooks/01_explore_data.ipynb
```
Then:
1. Update the `data_path` variable with your filename
2. Run the "Load Dataset" cell
3. Verify output shows row count and columns
4. Run the inspection cells (uncomment them)
5. Document your findings in the "Summary & Next Steps" section

#### ‚ö†Ô∏è Common Issues:
- **Missing columns**: Some datasets use different column names (e.g., "comments" vs "description")
- **Date formats**: May need parsing with different formats
- **Encoding issues**: Try `pd.read_csv(path, encoding='utf-8')` or `encoding='latin-1'`

---

### Task 2.2: Implement Data Cleaning Script

**Estimated Time**: 2-3 hours

#### Checklist:
- [ ] Create `ml/scripts/clean_data.py`
- [ ] Implement date/time parsing and normalization
- [ ] Implement location and coordinate validation
- [ ] Implement text cleaning
- [ ] Handle missing values
- [ ] Test script and verify output

#### üìÅ Files to Create:
- `ml/scripts/clean_data.py` (new - main script)

#### üìÅ Files Affected:
- `ml/data/processed/cleaned_sightings.parquet` (output)

#### üí° Implementation Guide:

**Step 1**: Create the script skeleton
```python
# ml/scripts/clean_data.py
import pandas as pd
import numpy as np
from pathlib import Path
from dateutil import parser
import re

def load_raw_data():
    """Load the raw dataset"""
    pass

def clean_dates(df):
    """Parse and normalize dates to UTC"""
    pass

def clean_locations(df):
    """Validate and clean lat/lon"""
    pass

def clean_text(df):
    """Clean description text"""
    pass

def handle_missing_values(df):
    """Drop or impute missing data"""
    pass

def main():
    """Run the full cleaning pipeline"""
    pass

if __name__ == "__main__":
    main()
```

**Step 2**: Implement each function following the PRD requirements

**Step 3**: Add logging and error handling

#### üéØ Acceptance Criteria:
- [ ] Script runs without errors: `python ml/scripts/clean_data.py`
- [ ] Output file created: `ml/data/processed/cleaned_sightings.parquet`
- [ ] Dates are in consistent format (ISO 8601 or UTC timestamps)
- [ ] Lat/lon are validated (within valid ranges)
- [ ] Text is cleaned (whitespace trimmed, basic normalization)
- [ ] Missing value strategy is documented
- [ ] Script logs: number of rows before/after, columns processed

#### ‚úÖ Verification Steps:
```bash
cd ml
source .venv/bin/activate
python scripts/clean_data.py

# Then verify output:
python -c "
import pandas as pd
df = pd.read_parquet('data/processed/cleaned_sightings.parquet')
print(f'Rows: {len(df):,}')
print(f'Columns: {list(df.columns)}')
print(df.head())
print(df.info())
"
```

#### üìö Learning Resources:
- [Pandas date parsing](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)
- [Handling missing data](https://pandas.pydata.org/docs/user_guide/missing_data.html)
- [Text cleaning in Python](https://realpython.com/python-string-formatting/)

---

### Task 2.3: Spatiotemporal Aggregation & Baseline Modeling

**Estimated Time**: 3-4 hours

#### Checklist:
- [ ] Create notebook `ml/notebooks/02_spatiotemporal_baseline.ipynb`
- [ ] Implement spatial grid (lat/lon binning or hexagons)
- [ ] Aggregate sightings by grid cell and time window
- [ ] Create features (year, month, grid coordinates)
- [ ] Train baseline prediction model
- [ ] Compute cell-level anomaly scores
- [ ] Export results

#### üìÅ Files to Create:
- `ml/notebooks/02_spatiotemporal_baseline.ipynb` (new)
- `ml/data/processed/grid_time_anomalies.parquet` (output)

#### üí° Implementation Guide:

**Spatial Gridding Options**:
1. **Simple lat/lon rounding** (easiest):
   ```python
   df['grid_lat'] = (df['latitude'] // 1.0) * 1.0  # 1¬∞ grid
   df['grid_lon'] = (df['longitude'] // 1.0) * 1.0
   ```

2. **Uber H3 hexagons** (more advanced):
   ```python
   import h3
   df['h3_cell'] = df.apply(lambda row: h3.geo_to_h3(row['lat'], row['lon'], 5), axis=1)
   ```

**Temporal Aggregation**:
```python
df['year_month'] = df['date'].dt.to_period('M')
```

**Baseline Model Features**:
- Historical count (autoregressive)
- Month/season
- Year trend
- Grid cell (one-hot or embedding)
- Optional: population density (external data)

**Model Options**:
- Start simple: Linear Regression
- Better: Gradient Boosting (XGBoost, LightGBM)
- Advanced: Poisson Regression (for count data)

#### üéØ Acceptance Criteria:
- [ ] Grid system defined (document cell size/resolution)
- [ ] Data aggregated by (grid_cell, time_window)
- [ ] Baseline model trained with reasonable metrics (R¬≤ > 0.3)
- [ ] Anomaly scores computed (z-scores or residuals)
- [ ] Results saved to `grid_time_anomalies.parquet`
- [ ] Notebook has visualizations of:
  - [ ] Sightings per cell (heatmap)
  - [ ] Predicted vs actual counts
  - [ ] Top anomalous cells

#### ‚úÖ Verification Steps:
```python
# In the notebook, verify:
import pandas as pd
anomalies = pd.read_parquet('../data/processed/grid_time_anomalies.parquet')

# Should have columns like:
# - grid_lat, grid_lon (or h3_cell)
# - year_month (or time_window)
# - actual_count
# - predicted_count
# - anomaly_score

print(anomalies.head())
print(f"Total grid cells: {anomalies['grid_cell'].nunique()}")
print(f"Top anomalies:\n{anomalies.nlargest(10, 'anomaly_score')}")
```

#### üìö Learning Resources:
- [Scikit-learn regression](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)
- [Time series features](https://www.kaggle.com/code/ryanholbrook/time-series-as-features)
- [Anomaly detection basics](https://scikit-learn.org/stable/modules/outlier_detection.html)

---

### Task 2.4: Text Embeddings & Clustering

**Estimated Time**: 2-3 hours

#### Checklist:
- [ ] Create notebook `ml/notebooks/03_text_clusters.ipynb`
- [ ] Generate sentence embeddings for descriptions
- [ ] Experiment with clustering algorithms
- [ ] Choose optimal number of clusters
- [ ] Assign human-readable labels to clusters
- [ ] Save cluster assignments and labels

#### üìÅ Files to Create:
- `ml/notebooks/03_text_clusters.ipynb` (new)
- `ml/data/processed/cluster_labels.json` (output)
- `ml/data/processed/sightings_with_clusters.parquet` (output)

#### üí° Implementation Guide:

**Step 1: Generate Embeddings**
```python
from sentence_transformers import SentenceTransformer

# Choose a model
model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, good quality

# Generate embeddings
descriptions = df['description'].fillna('').tolist()
embeddings = model.encode(descriptions, show_progress_bar=True)
```

**Step 2: Clustering**
```python
# Option A: KMeans (simpler, requires choosing k)
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=15, random_state=42)
clusters = kmeans.fit_predict(embeddings)

# Option B: HDBSCAN (auto-determines clusters)
import hdbscan
clusterer = hdbscan.HDBSCAN(min_cluster_size=100)
clusters = clusterer.fit_predict(embeddings)
```

**Step 3: Label Clusters**
```python
# For each cluster, extract top words/phrases
from sklearn.feature_extraction.text import TfidfVectorizer

def label_cluster(cluster_id, df, cluster_col='cluster_id'):
    cluster_texts = df[df[cluster_col] == cluster_id]['description']
    # Use TF-IDF to find characteristic words
    # Manually inspect and assign label
    return "bright_lights"  # example
```

**Suggested Cluster Labels** (adjust based on your data):
- "bright_lights"
- "triangular_craft"
- "orb_sphere"
- "fast_moving_object"
- "hovering_object"
- "formation_multiple"
- "cigar_cylinder"
- "disk_saucer"
- "flashing_lights"
- "silent_movement"

#### üéØ Acceptance Criteria:
- [ ] Embeddings generated for all descriptions
- [ ] Clustering algorithm applied (KMeans or HDBSCAN)
- [ ] Number of clusters is reasonable (10-30)
- [ ] Each cluster has human-readable label
- [ ] Cluster labels saved to JSON: `{"0": "bright_lights", "1": "triangular_craft", ...}`
- [ ] Original dataset updated with `cluster_id` column
- [ ] Notebook shows:
  - [ ] Cluster size distribution
  - [ ] Sample descriptions per cluster
  - [ ] Visualization (t-SNE or UMAP)

#### ‚úÖ Verification Steps:
```python
# Verify cluster labels
import json
with open('../data/processed/cluster_labels.json') as f:
    labels = json.load(f)
print(f"Number of clusters: {len(labels)}")
print(f"Labels: {labels}")

# Verify updated dataset
df = pd.read_parquet('../data/processed/sightings_with_clusters.parquet')
print(f"Cluster distribution:\n{df['cluster_id'].value_counts()}")
```

#### üìö Learning Resources:
- [Sentence Transformers documentation](https://www.sbert.net/)
- [KMeans clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means)
- [HDBSCAN guide](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)
- [Dimensionality reduction for viz](https://umap-learn.readthedocs.io/en/latest/)

---

### Task 2.5: Per-Report Anomaly Model

**Estimated Time**: 2-3 hours

#### Checklist:
- [ ] Create notebook `ml/notebooks/04_per_report_anomaly.ipynb`
- [ ] Merge data: cleaned sightings + cell anomalies + clusters
- [ ] Engineer features for anomaly detection
- [ ] Train Isolation Forest model
- [ ] Generate per-report anomaly scores
- [ ] Export final dataset with all scores

#### üìÅ Files to Create:
- `ml/notebooks/04_per_report_anomaly.ipynb` (new)
- `ml/data/processed/sightings_with_scores.parquet` (output - final dataset)

#### üí° Implementation Guide:

**Step 1: Merge Data Sources**
```python
# Load all processed data
cleaned = pd.read_parquet('../data/processed/cleaned_sightings.parquet')
clusters = pd.read_parquet('../data/processed/sightings_with_clusters.parquet')
grid_anomalies = pd.read_parquet('../data/processed/grid_time_anomalies.parquet')

# Merge (adjust keys based on your data)
df = cleaned.merge(clusters[['id', 'cluster_id']], on='id')
df = df.merge(grid_anomalies, on=['grid_lat', 'grid_lon', 'year_month'])
```

**Step 2: Feature Engineering**
```python
# Numeric features
features = []

# 1. Duration (normalize/log scale)
df['duration_log'] = np.log1p(df['duration_seconds'])
features.append('duration_log')

# 2. Time of day
df['hour'] = df['datetime'].dt.hour
features.append('hour')

# 3. Month/season
df['month'] = df['datetime'].dt.month
features.append('month')

# 4. Cell anomaly score
features.append('cell_anomaly_score')

# 5. Cluster ID (one-hot encode)
cluster_dummies = pd.get_dummies(df['cluster_id'], prefix='cluster')
features.extend(cluster_dummies.columns.tolist())

# 6. Shape (one-hot encode)
shape_dummies = pd.get_dummies(df['shape'], prefix='shape')
features.extend(shape_dummies.columns.tolist())

# Combine all features
X = pd.concat([df[['duration_log', 'hour', 'month', 'cell_anomaly_score']],
               cluster_dummies, shape_dummies], axis=1)
```

**Step 3: Train Isolation Forest**
```python
from sklearn.ensemble import IsolationForest

# Train model
iso_forest = IsolationForest(
    contamination=0.1,  # expect 10% anomalies
    random_state=42,
    n_jobs=-1
)
anomaly_labels = iso_forest.fit_predict(X)

# Convert to scores (0-1 range)
anomaly_scores = iso_forest.score_samples(X)
# Normalize to 0-1 (higher = more anomalous)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df['anomaly_score_report'] = scaler.fit_transform(
    -anomaly_scores.reshape(-1, 1)  # negate so higher is more anomalous
)
```

#### üéØ Acceptance Criteria:
- [ ] All data sources merged successfully
- [ ] Feature matrix created with at least 5 features
- [ ] Isolation Forest trained without errors
- [ ] Every sighting has `anomaly_score_report` (0-1 scale)
- [ ] Final dataset saved with columns:
  - [ ] All original fields (date, location, description, etc.)
  - [ ] `cluster_id` and `cluster_label`
  - [ ] `cell_anomaly_score`
  - [ ] `anomaly_score_report`
- [ ] Notebook shows:
  - [ ] Feature importance or top anomalous reports
  - [ ] Distribution of anomaly scores
  - [ ] Examples of high-anomaly reports

#### ‚úÖ Verification Steps:
```python
# Verify final dataset
df = pd.read_parquet('../data/processed/sightings_with_scores.parquet')

print(f"Total sightings: {len(df):,}")
print(f"Columns: {df.columns.tolist()}")
print(f"\nAnomaly score distribution:")
print(df['anomaly_score_report'].describe())

# Top anomalies
print("\nTop 10 most anomalous sightings:")
print(df.nlargest(10, 'anomaly_score_report')[
    ['datetime', 'location', 'anomaly_score_report', 'cluster_label', 'description']
])
```

#### üìö Learning Resources:
- [Isolation Forest explained](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)
- [Feature engineering guide](https://www.kaggle.com/learn/feature-engineering)
- [One-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)

---

### Task 2.6: Update Exploration Notebook with Real Data

**Estimated Time**: 30 minutes

#### Checklist:
- [ ] Uncomment all cells in `01_explore_data.ipynb`
- [ ] Run full notebook with actual dataset
- [ ] Document findings in markdown cells
- [ ] Generate and save key visualizations
- [ ] Commit notebook with outputs

#### üéØ Acceptance Criteria:
- [ ] Notebook runs completely without errors
- [ ] All visualizations render
- [ ] Summary section filled out with insights
- [ ] Notebook saved with outputs visible

---

## ‚è∏Ô∏è Phase 3: Static Data Export & Tiling

**Status**: ‚è∏Ô∏è Pending
**Estimated Time**: 2-3 hours
**Progress**: 0/2 tasks complete

### Task 3.1: Design Tile Format & Zoom Strategy

**Estimated Time**: 30-45 minutes

#### Checklist:
- [ ] Define grid/tile bucketing strategy
- [ ] Design JSON schema for tiles
- [ ] Design metadata JSON structure
- [ ] Document tile naming convention
- [ ] Test with sample data

#### üìÅ Files to Create/Update:
- Document in `ml/README.md` (update)

#### üí° Implementation Options:

**Option A: Simple Grid Buckets** (Recommended for v1)
```
Tile naming: lat{LAT}_lon{LON}.json
Example: lat37_lon-122.json (1¬∞ x 1¬∞ bucket)
```

**Option B: Mapbox-style Z/X/Y Tiles** (More complex, better for large scale)
```
Tile naming: z{ZOOM}/x{X}_y{Y}.json
Example: z5/x10_y15.json
```

**JSON Schema Example**:
```json
{
  "grid_id": "lat37_lon-122",
  "bounds": {
    "north": 38.0,
    "south": 37.0,
    "east": -121.0,
    "west": -122.0
  },
  "count": 1543,
  "sightings": [
    {
      "id": "abc123",
      "lat": 37.7749,
      "lon": -122.4194,
      "timestamp": "2020-06-15T22:30:00Z",
      "cluster_id": 3,
      "cluster_label": "bright_lights",
      "anomaly_score_report": 0.85,
      "cell_anomaly_score": 1.2,
      "shape": "light",
      "duration": 120,
      "description": "Bright light moving rapidly..."
    }
  ]
}
```

**Metadata Schema** (`cluster_labels.json`):
```json
{
  "0": "bright_lights",
  "1": "triangular_craft",
  "2": "orb_sphere"
}
```

**Metadata Schema** (`global_stats.json`):
```json
{
  "total_sightings": 87543,
  "date_range": {
    "start": "1990-01-01",
    "end": "2024-12-31"
  },
  "clusters": {
    "0": { "label": "bright_lights", "count": 23451 },
    "1": { "label": "triangular_craft", "count": 15234 }
  },
  "shapes": {
    "light": 25000,
    "triangle": 18000
  }
}
```

#### üéØ Acceptance Criteria:
- [ ] Tile bucketing strategy documented
- [ ] JSON schemas documented with examples
- [ ] Tile size reasonable (< 500KB per tile ideally)
- [ ] Metadata structure defined

---

### Task 3.2: Implement Export Script

**Estimated Time**: 2-3 hours

#### Checklist:
- [ ] Create `ml/scripts/export_tiles.py`
- [ ] Implement tile generation logic
- [ ] Implement metadata export
- [ ] Add progress bars and logging
- [ ] Test with full dataset
- [ ] Verify output files

#### üìÅ Files to Create:
- `ml/scripts/export_tiles.py` (new)

#### üìÅ Output Files:
- `app/public/data/tiles/*.json` (many files)
- `app/public/data/metadata/cluster_labels.json`
- `app/public/data/metadata/global_stats.json`

#### üí° Implementation Skeleton:
```python
# ml/scripts/export_tiles.py
import pandas as pd
import json
from pathlib import Path
from collections import defaultdict
from tqdm import tqdm

def load_processed_data():
    """Load final processed dataset"""
    return pd.read_parquet('data/processed/sightings_with_scores.parquet')

def assign_tiles(df):
    """Assign each sighting to a tile"""
    df['tile_id'] = df.apply(
        lambda row: f"lat{int(row['latitude'])}_lon{int(row['longitude'])}",
        axis=1
    )
    return df

def generate_tiles(df, output_dir):
    """Generate JSON tile files"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    for tile_id, group in tqdm(df.groupby('tile_id')):
        tile_data = {
            'grid_id': tile_id,
            'count': len(group),
            'sightings': group.to_dict('records')
        }

        with open(output_dir / f"{tile_id}.json", 'w') as f:
            json.dump(tile_data, f, default=str)

def export_metadata(df, output_dir):
    """Export cluster labels and global stats"""
    # Implementation here
    pass

def main():
    df = load_processed_data()
    df = assign_tiles(df)
    generate_tiles(df, '../app/public/data/tiles')
    export_metadata(df, '../app/public/data/metadata')

if __name__ == "__main__":
    main()
```

#### üéØ Acceptance Criteria:
- [ ] Script runs: `python ml/scripts/export_tiles.py`
- [ ] Tiles generated in `app/public/data/tiles/`
- [ ] Each tile is valid JSON
- [ ] Metadata files created in `app/public/data/metadata/`
- [ ] Script logs:
  - [ ] Number of tiles generated
  - [ ] Total sightings exported
  - [ ] File sizes
- [ ] Sample tile verified to load in browser

#### ‚úÖ Verification Steps:
```bash
cd ml
python scripts/export_tiles.py

# Check output
ls -lh ../app/public/data/tiles/ | head
cat ../app/public/data/tiles/lat37_lon-122.json | jq '.'  # if jq installed
cat ../app/public/data/metadata/cluster_labels.json
cat ../app/public/data/metadata/global_stats.json
```

---

## ‚è∏Ô∏è Phase 4: Core Frontend Map & Filters

**Status**: ‚è∏Ô∏è Pending
**Estimated Time**: 8-10 hours
**Progress**: 0/5 tasks complete

### Task 4.1: Integrate Map Library

**Estimated Time**: 1-2 hours

#### Checklist:
- [ ] Choose map library (Mapbox GL or deck.gl)
- [ ] Install dependencies
- [ ] Get API key (if needed)
- [ ] Create MapView component
- [ ] Test basic map rendering

#### üìÅ Files to Create:
- `app/components/MapView.tsx` (new)
- `app/.env.local` (new - for API keys)

#### üí° Implementation Guide:

**Option A: Mapbox GL JS** (Recommended - easier)
```bash
npm install mapbox-gl @types/mapbox-gl
npm install react-map-gl
```

**Option B: deck.gl** (More powerful for large datasets)
```bash
npm install deck.gl @deck.gl/react
```

**Mapbox Component Example**:
```tsx
// app/components/MapView.tsx
'use client';

import { useRef, useEffect } from 'react';
import mapboxgl from 'mapbox-gl';
import 'mapbox-gl/dist/mapbox-gl.css';

mapboxgl.accessToken = process.env.NEXT_PUBLIC_MAPBOX_TOKEN || '';

export default function MapView() {
  const mapContainer = useRef<HTMLDivElement>(null);
  const map = useRef<mapboxgl.Map | null>(null);

  useEffect(() => {
    if (map.current) return; // Initialize map only once

    map.current = new mapboxgl.Map({
      container: mapContainer.current!,
      style: 'mapbox://styles/mapbox/dark-v11',
      center: [-95.7129, 37.0902], // Center of US
      zoom: 4
    });
  }, []);

  return (
    <div
      ref={mapContainer}
      className="w-full h-screen"
    />
  );
}
```

**Get Mapbox Token**:
1. Sign up at https://account.mapbox.com/
2. Create access token
3. Add to `.env.local`:
```
NEXT_PUBLIC_MAPBOX_TOKEN=your_token_here
```

#### üéØ Acceptance Criteria:
- [ ] Map library installed
- [ ] MapView component created
- [ ] Map renders on page
- [ ] Can pan and zoom
- [ ] No console errors

#### ‚úÖ Verification:
Update `app/page.tsx` to include map:
```tsx
import MapView from '@/components/MapView';

export default function Home() {
  return <MapView />;
}
```

Run `npm run dev` and check http://localhost:3000

---

### Task 4.2: Load and Render Static Tiles

**Estimated Time**: 2-3 hours

#### Checklist:
- [ ] Create tile loading utility
- [ ] Implement viewport-based tile selection
- [ ] Load tile JSON files
- [ ] Render points on map
- [ ] Test with multiple tiles

#### üìÅ Files to Create:
- `app/lib/tileLoader.ts` (new - utility)
- `app/types/sighting.ts` (new - TypeScript types)

#### üìÅ Files to Update:
- `app/components/MapView.tsx` (update - add data layer)

#### üí° Implementation:

**TypeScript Types**:
```tsx
// app/types/sighting.ts
export interface Sighting {
  id: string;
  lat: number;
  lon: number;
  timestamp: string;
  cluster_id: number;
  cluster_label: string;
  anomaly_score_report: number;
  cell_anomaly_score: number;
  shape: string;
  duration: number;
  description: string;
}

export interface Tile {
  grid_id: string;
  count: number;
  sightings: Sighting[];
}
```

**Tile Loader**:
```tsx
// app/lib/tileLoader.ts
import { Tile } from '@/types/sighting';

export async function loadTile(tileId: string): Promise<Tile | null> {
  try {
    const response = await fetch(`/data/tiles/${tileId}.json`);
    if (!response.ok) return null;
    return await response.json();
  } catch (error) {
    console.error(`Failed to load tile ${tileId}:`, error);
    return null;
  }
}

export function getTilesInView(bounds: {
  north: number;
  south: number;
  east: number;
  west: number;
}): string[] {
  const tiles: string[] = [];

  for (let lat = Math.floor(bounds.south); lat <= Math.ceil(bounds.north); lat++) {
    for (let lon = Math.floor(bounds.west); lon <= Math.ceil(bounds.east); lon++) {
      tiles.push(`lat${lat}_lon${lon}`);
    }
  }

  return tiles;
}
```

#### üéØ Acceptance Criteria:
- [ ] Tiles load based on map viewport
- [ ] Points render on map
- [ ] Multiple tiles load correctly
- [ ] Map updates when panning

---

### Task 4.3: Add Basic Filters (Cluster, Anomaly Threshold)

**Estimated Time**: 2-3 hours

#### Checklist:
- [ ] Create FiltersPanel component
- [ ] Load cluster labels from metadata
- [ ] Implement cluster multi-select
- [ ] Implement anomaly score slider
- [ ] Connect filters to map rendering
- [ ] Test filter combinations

#### üìÅ Files to Create:
- `app/components/FiltersPanel.tsx` (new)
- `app/hooks/useFilters.ts` (new - custom hook)

---

### Task 4.4: Implement Time Slider

**Estimated Time**: 2-3 hours

#### Checklist:
- [ ] Create TimeSlider component
- [ ] Load date range from metadata
- [ ] Implement date range selection
- [ ] Add play/pause animation (optional)
- [ ] Filter sightings by time
- [ ] Test time filtering

#### üìÅ Files to Create:
- `app/components/TimeSlider.tsx` (new)

---

### Task 4.5: Optimize Map Performance

**Estimated Time**: 1-2 hours

#### Checklist:
- [ ] Implement clustering for low zoom levels
- [ ] Add debouncing for tile loading
- [ ] Optimize re-renders
- [ ] Test with full dataset
- [ ] Monitor performance metrics

---

## ‚è∏Ô∏è Phase 5: Detail Panel, Insights & Polish

**Status**: ‚è∏Ô∏è Pending
**Estimated Time**: 4-6 hours
**Progress**: 0/4 tasks complete

### Task 5.1: Sighting Detail Panel

**Estimated Time**: 2-3 hours

#### Checklist:
- [ ] Create SightingDetailPanel component
- [ ] Implement click handler on map points
- [ ] Display sighting details
- [ ] Show anomaly score explanation
- [ ] Add close functionality
- [ ] Add slide-in animation

#### üìÅ Files to Create:
- `app/components/SightingDetailPanel.tsx` (new)

---

### Task 5.2: Basic Insights Screen (Stretch)

**Estimated Time**: 2-3 hours

#### Checklist:
- [ ] Create insights page
- [ ] Add charts for temporal trends
- [ ] Add charts for cluster distribution
- [ ] Load global stats
- [ ] Add navigation

---

### Task 5.3: UI Polish

**Estimated Time**: 1-2 hours

#### Checklist:
- [ ] Refine color palette
- [ ] Add loading states
- [ ] Add error states
- [ ] Improve mobile responsiveness
- [ ] Add legend for anomaly colors
- [ ] Polish typography and spacing

---

### Task 5.4: Bookmarkable State (Stretch)

**Estimated Time**: 1 hour

#### Checklist:
- [ ] Encode filters in URL
- [ ] Encode map viewport in URL
- [ ] Parse URL on load
- [ ] Test sharing links

---

## ‚è∏Ô∏è Phase 6: Testing, Deployment & Documentation

**Status**: ‚è∏Ô∏è Pending
**Estimated Time**: 4-6 hours
**Progress**: 0/4 tasks complete

### Task 6.1: Basic Frontend Tests

**Estimated Time**: 2-3 hours

#### Checklist:
- [ ] Setup Jest and React Testing Library
- [ ] Write tests for FiltersPanel
- [ ] Write tests for MapView (basic)
- [ ] Write tests for data loading
- [ ] Run tests: `npm test`

---

### Task 6.2: Manual QA Checklist

**Estimated Time**: 1-2 hours

#### Checklist:
- [ ] Create QA_CHECKLIST.md
- [ ] Test all filters
- [ ] Test time slider
- [ ] Test detail panel
- [ ] Test on different browsers
- [ ] Test on mobile
- [ ] Fix identified bugs

---

### Task 6.3: Deployment to Vercel

**Estimated Time**: 30-60 minutes

#### Checklist:
- [ ] Create Vercel account
- [ ] Connect GitHub repo
- [ ] Configure environment variables
- [ ] Deploy to production
- [ ] Test deployed app
- [ ] Configure custom domain (optional)

**Deployment Steps**:
```bash
# Option 1: Vercel CLI
npm i -g vercel
vercel login
vercel

# Option 2: Vercel Dashboard
# 1. Visit vercel.com
# 2. Import GitHub repo
# 3. Configure build settings
# 4. Deploy
```

---

### Task 6.4: Final Documentation

**Estimated Time**: 1-2 hours

#### Checklist:
- [ ] Update README with deployment URL
- [ ] Add architecture diagram
- [ ] Document data pipeline commands
- [ ] Add screenshots to README
- [ ] Create CHANGELOG.md
- [ ] Update ml/README with final notes
- [ ] Add contributor guidelines

---

## üìã Quick Reference Commands

### Frontend Development
```bash
cd app
npm run dev        # Start dev server
npm run build      # Production build
npm run lint       # Run linter
npm test           # Run tests
```

### ML Pipeline
```bash
cd ml
source .venv/bin/activate                      # Activate venv
jupyter notebook                                # Start Jupyter
python scripts/clean_data.py                   # Clean data
python scripts/export_tiles.py                 # Export tiles
```

### Git Workflow
```bash
git status                                      # Check status
git add .                                       # Stage all
git commit -m "feat: add feature description"  # Commit
git push origin main                            # Push
```

---

## üéØ Milestones

- **M1 (Current)**: Basic structure & data pipeline started ‚Üí Target: Complete Phase 2
- **M2**: ML pipeline outputs ready ‚Üí Target: Complete Phase 3
- **M3**: Minimal map rendering ‚Üí Target: Complete Task 4.1-4.2
- **M4 (Demo-Ready v1)**: Filters, time slider & detail panel ‚Üí Target: Complete Phase 4-5
- **M5 (Production)**: Polish, insights & deployment ‚Üí Target: Complete Phase 6

---

## üìä Time Tracking

| Phase | Estimated | Actual | Status |
|-------|-----------|--------|--------|
| Phase 1 | 1h | 1h | ‚úÖ Complete |
| Phase 2 | 10h | - | üîÑ In Progress |
| Phase 3 | 3h | - | ‚è∏Ô∏è Pending |
| Phase 4 | 10h | - | ‚è∏Ô∏è Pending |
| Phase 5 | 6h | - | ‚è∏Ô∏è Pending |
| Phase 6 | 6h | - | ‚è∏Ô∏è Pending |
| **Total** | **36h** | **1h** | **3% Complete** |

---

## ‚ö†Ô∏è Common Issues & Solutions

### ML Pipeline Issues

**Issue**: Out of memory when processing large datasets
**Solution**: Process in chunks using `pd.read_csv(chunksize=10000)`

**Issue**: Embeddings taking too long
**Solution**: Use smaller model or reduce dataset size for testing

**Issue**: Clusters not meaningful
**Solution**: Adjust number of clusters, try HDBSCAN, or improve text preprocessing

### Frontend Issues

**Issue**: Map not rendering
**Solution**: Check API token, check console for errors, verify mapbox-gl CSS imported

**Issue**: Tiles not loading
**Solution**: Check file paths, verify JSON is valid, check browser network tab

**Issue**: App slow with many points
**Solution**: Implement clustering, limit points per zoom level, use WebGL rendering

---

## üéì Learning Resources

- **Next.js**: https://nextjs.org/docs
- **Mapbox GL**: https://docs.mapbox.com/mapbox-gl-js/
- **Pandas**: https://pandas.pydata.org/docs/
- **Scikit-learn**: https://scikit-learn.org/stable/
- **Sentence Transformers**: https://www.sbert.net/
- **React Testing Library**: https://testing-library.com/react

---

**Last Updated**: 2025-11-26
**Current Phase**: Phase 2 - Data Ingestion & ML Pipeline
**Next Task**: Task 2.1 - Download and Inspect UAP Dataset

Good luck building! üöÄ
